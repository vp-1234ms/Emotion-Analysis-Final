{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Deng, Sinuo, et al. \"Simple but powerful, a language-supervised method for image emotion classification.\" IEEE Transactions on Affective Computing 14.4 (2022): 3317-3331."
      ],
      "metadata": {
        "id": "8tJo_JyMCuEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WKbfojC2Fed"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Jsunuaxjt7",
        "outputId": "95093dcc-7a9c-4426-f0fe-0ef22fb220de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un5ZtY5BxoJF"
      },
      "outputs": [],
      "source": [
        "train = '/content/drive/My Drive/PROJECT/Image Data/train'\n",
        "test = '/content/drive/My Drive/PROJECT/Image Data/test'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Dropout , Lambda, Flatten\n",
        "from keras.optimizers import Adam ,RMSprop\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import  backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n"
      ],
      "metadata": {
        "id": "Qyw10uuMFYH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import  Sequential\n",
        "from keras.layers.core import  Lambda , Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import BatchNormalization, Convolution2D , MaxPooling2D"
      ],
      "metadata": {
        "id": "V8en7EgiFknX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coWM8P-6x8wa"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    width_shift_range = 0.1,        # Randomly shift the width of images by up to 10%\n",
        "    height_shift_range = 0.1,       # Randomly shift the height of images by up to 10%\n",
        "    horizontal_flip = True,         # Flip images horizontally at random\n",
        "    rescale = 1./255,               # Rescale pixel values to be between 0 and 1\n",
        "    validation_split = 0.2          # Set aside 20% of the data for validation\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,               # Rescale pixel values to be between 0 and 1\n",
        "    validation_split = 0.2          # Set aside 20% of the data for validation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJdhYrEBx_Xs",
        "outputId": "03781889-c108-4ac0-fc06-63ddf64ca8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 22968 images belonging to 7 classes.\n",
            "Found 1432 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory = train,           # Directory containing the training data\n",
        "    target_size = (48, 48),          # Resizes all images to 48x48 pixels\n",
        "    batch_size = 64,                 # Number of images per batch\n",
        "    color_mode = \"grayscale\",        # Converts the images to grayscale\n",
        "    class_mode = \"categorical\",      # Classifies the images into 7 categories\n",
        "    subset = \"training\"              # Uses the training subset of the data\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory = test,            # Directory containing the validation data\n",
        "    target_size = (48, 48),          # Resizes all images to 48x48 pixels\n",
        "    batch_size = 64,                 # Number of images per batch\n",
        "    color_mode = \"grayscale\",        # Converts the images to grayscale\n",
        "    class_mode = \"categorical\",      # Classifies the images into 7 categories\n",
        "    subset = \"validation\"            # Uses the validation subset of the data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textwrap import wrap"
      ],
      "metadata": {
        "id": "dZUlyapgFMUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = Input(shape=(1920,))\n",
        "input2 = Input(shape=(max_length,))\n",
        "\n",
        "img_features = Dense(256, activation='relu')(input1)\n",
        "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
        "\n",
        "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
        "merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
        "sentence_features = LSTM(256)(merged)\n",
        "x = Dropout(0.5)(sentence_features)\n",
        "x = add([x, img_features])\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "caption_model = Model(inputs=[input1,input2], outputs=output)\n",
        "caption_model.compile(loss='categorical_crossentropy',optimizer='adam')"
      ],
      "metadata": {
        "id": "YFuGAP80F2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model.summary()"
      ],
      "metadata": {
        "id": "wF3clir2F9Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwgNWPeXF99q",
        "outputId": "403e5777-a79e-44a1-8f75-b699e1b29f17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 1920)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          491776      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 36)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1, 256)       0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 36, 256)      2172160     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 37, 256)      0           reshape[0][0]                    \n",
            "                                                                 embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256)          0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dropout[0][0]                    \n",
            "                                                                 dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          32896       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 8485)         1094565     dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,316,709\n",
            "Trainable params: 4,316,709\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = caption_model.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOdkF8W_GMwD",
        "outputId": "bc8d7d1e-b59f-4e0f-95ed-a4ef98dcc645"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train on 11227 samples, validate on 2807 samples\n",
            "Epoch 1/20\n",
            "11227/11227 [==============================] - 9s 798us/sample - loss: 1.2029 - acc: 0.1884 - val_loss: 0.9420 - val_acc: 0.1462\n",
            "Epoch 2/20\n",
            "11227/11227 [==============================] - 6s 512us/sample - loss: 0.7826 - acc: 0.2120 - val_loss: 0.8539 - val_acc: 0.2072\n",
            "Epoch 3/20\n",
            "11227/11227 [==============================] - 5s 414us/sample - loss: 0.6219 - acc: 0.2555 - val_loss: 0.7070 - val_acc: 0.2474\n",
            "Epoch 4/20\n",
            "11227/11227 [==============================] - 5s 420us/sample - loss: 0.4614 - acc: 0.3072 - val_loss: 0.7136 - val_acc: 0.2874\n",
            "Epoch 5/20\n",
            "11227/11227 [==============================] - 5s 438us/sample - loss: 0.3462 - acc: 0.3529 - val_loss: 0.7459 - val_acc: 0.3414\n",
            "Epoch 6/20\n",
            "11227/11227 [==============================] - 5s 436us/sample - loss: 0.2944 - acc: 0.3809 - val_loss: 0.6843 - val_acc: 0.3213\n",
            "Epoch 7/20\n",
            "11227/11227 [==============================] - 5s 426us/sample - loss: 0.1656 - acc: 0.4298 - val_loss: 0.7731 - val_acc: 0.4020\n",
            "Epoch 8/20\n",
            "11227/11227 [==============================] - 5s 419us/sample - loss: 0.1188 - acc: 0.4670 - val_loss: 0.7210 - val_acc: 0.4891\n",
            "Epoch 9/20\n",
            "11227/11227 [==============================] - 5s 423us/sample - loss: 0.0765 - acc: 0.5117 - val_loss: 0.8328 - val_acc: 0.5012\n",
            "Epoch 10/20\n",
            "11227/11227 [==============================] - 5s 420us/sample - loss: 0.0612 - acc: 0.5362 - val_loss: 0.8919 - val_acc: 0.5284\n",
            "Epoch 11/20\n",
            "11227/11227 [==============================] - 5s 424us/sample - loss: 0.0363 - acc: 0.5542 - val_loss: 0.9345 - val_acc: 0.5337\n",
            "Epoch 12/20\n",
            "11227/11227 [==============================] - 5s 419us/sample - loss: 0.0269 - acc: 0.5867 - val_loss: 1.0022 - val_acc: 0.5705\n",
            "Epoch 13/20\n",
            "11227/11227 [==============================] - 5s 416us/sample - loss: 0.0203 - acc: 0.6071 - val_loss: 1.0688 - val_acc: 0.6005\n",
            "Epoch 14/20\n",
            "11227/11227 [==============================] - 5s 421us/sample - loss: 0.0166 - acc: 0.6283 - val_loss: 1.0574 - val_acc: 0.6059\n",
            "Epoch 15/20\n",
            "11227/11227 [==============================] - 5s 416us/sample - loss: 0.0140 - acc: 0.6586 - val_loss: 1.0373 - val_acc: 0.7041\n",
            "Epoch 16/20\n",
            "11227/11227 [==============================] - 5s 420us/sample - loss: 0.0099 - acc: 0.6992 - val_loss: 1.1272 - val_acc: 0.7052\n",
            "Epoch 17/20\n",
            "11227/11227 [==============================] - 5s 435us/sample - loss: 0.0110 - acc: 0.7288 - val_loss: 1.1072 - val_acc: 0.7159\n",
            "Epoch 18/20\n",
            "11227/11227 [==============================] - 5s 432us/sample - loss: 0.0701 - acc: 0.7828 - val_loss: 1.1298 - val_acc: 0.7851\n",
            "Epoch 19/20\n",
            "11227/11227 [==============================] - 6s 500us/sample - loss: 0.1186 - acc: 0.7634 - val_loss: 0.9874 - val_acc: 0.7724\n",
            "Epoch 20/20\n",
            "11227/11227 [==============================] - 5s 487us/sample - loss: 0.0193 - acc: 0.7967 - val_loss: 1.0128 - val_acc: 0.7944\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQ7UIZJZHS7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNOXbFfhHS5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ps_N3NbZHS3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNPgviRoHS2e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}