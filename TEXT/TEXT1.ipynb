{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## N. Shelke, S. Chaudhury, S. Chakrabarti, S. L. Bangare, G. Yogapriya, and P. Pandey, “An efficient way of text-based emotion analysis from social media using lra-dnn,” Neuroscience Informatics, vol. 2, no. 3, p. 100048, 2022."
      ],
      "metadata": {
        "id": "UIM5lFV1oLS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import RobertaModel, RobertaTokenizerFast\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "metadata": {
        "id": "WD4KGdNrjgXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GphlNEYUstzg",
        "outputId": "aed0e9b9-aa15-4ec2-b6fb-5a3eb2615e29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gjcQtIF4jjZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/content/drive/My Drive/PROJECT/Text Data/train.tsv'\n",
        "path2 = '/content/drive/My Drive/PROJECT/Text Data/dev.tsv'"
      ],
      "metadata": {
        "id": "CtP5RNsc1LDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(path1, sep='\\t', header=None, names=['Text', 'Class', 'ID'])\n",
        "df_dev = pd.read_csv(path2, sep='\\t', header=None, names=['Text', 'Class', 'ID'])"
      ],
      "metadata": {
        "id": "muS4hSeRyTfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Load the dataset\n",
        "train_df = pd.read_csv(path1 + 'train.txt', sep=';', names=['content', 'sentiment'], header=0)\n",
        "test_df = pd.read_csv(path2 + 'test.txt', sep=';', names=['content', 'sentiment'], header=0)\n",
        "valid_df = pd.read_csv(path1 + 'val.txt', sep=';', names=['content', 'sentiment'], header=0)\n",
        "df_train = pd.concat([train_df, test_df, valid_df], axis=0)\n",
        "\n",
        "#Split the data into train, test, and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_train['content'], df_train['sentiment'], test_size=0.2, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "#Oversample the minority classes using RandomOverSampler\n",
        "ros = RandomOverSampler()\n",
        "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1))\n",
        "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns=['content', 'sentiment'])\n",
        "\n",
        "#Encode the labels\n",
        "encoder = LabelEncoder()\n",
        "y_train_encoded = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "y_valid_encoded = encoder.transform(np.array(y_valid).reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(np.array(y_test).reshape(-1, 1))"
      ],
      "metadata": {
        "id": "b5ghmWbVkVvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the input data\n",
        "MAX_LEN = 128\n",
        "\n",
        "class EmotionsDataset(Dataset):\n",
        "    def __init__(self, data, labels, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.data[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "ZEs38alDkabq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = EmotionsDataset(train_os['content'].values, y_train_encoded, tokenizer, MAX_LEN)\n",
        "valid_dataset = EmotionsDataset(X_valid.values, y_valid_encoded, tokenizer, MAX_LEN)\n",
        "test_dataset = EmotionsDataset(X_test.values, y_test_encoded, tokenizer, MAX_LEN)\n",
        "\n",
        "#Create data loaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "749IAKmWkdaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the classification model\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "#Set hyperparameters\n",
        "num_classes = len(encoder.classes_)\n",
        "lr = 2e-5\n",
        "num_epochs = 4\n",
        "\n",
        "#Initialize the classifier\n",
        "classifier = EmotionClassifier(roberta_model, num_classes).to(device)\n"
      ],
      "metadata": {
        "id": "E_IW076Skfm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_train:\n",
        "    #Set optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = classifier(input_ids, attention_mask)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                batch_acc = train_correct / total\n",
        "                batch_loss = train_loss / (batch_idx + 1)\n",
        "                print(f\"Epoch {epoch + 1}/{num_epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {batch_loss:.4f} | Accuracy: {batch_acc:.4f}\")\n",
        "\n",
        "        train_accuracy = train_correct / total\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "classifier.eval()\n",
        "valid_loss = 0.0\n",
        "valid_correct = 0\n",
        "total = 0\n"
      ],
      "metadata": {
        "id": "cto89u_Pkk5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = classifier(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        valid_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "    valid_accuracy = valid_correct / total\n",
        "    valid_loss /= len(valid_loader)\n",
        "\n",
        "# Print training and validation results\n",
        "print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "print(f'Training Loss: {train_loss:.4f} | Training Accuracy: {train_accuracy:.4f}')\n",
        "print(f'Validation Loss: {valid_loss:.4f} | Validation Accuracy: {valid_accuracy:.4f}')\n",
        "print('------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7o6wQdlkuFH",
        "outputId": "5177828c-70a3-4853-ca0d-8ba300984115"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/16 | Batch 0/1009 | Loss: 0.4779 | Accuracy: 0.4213\n",
            "Epoch 1/16 | Batch 100/1009 | Loss: 0.4779 | Accuracy: 0.4513\n",
            "Epoch 1/16 | Batch 200/1009 | Loss: 0.4724 | Accuracy: 0.4833\n",
            "Epoch 1/16 | Batch 300/1009 | Loss: 0.4703 | Accuracy: 0.5093\n",
            "Epoch 1/16 | Batch 400/1009 | Loss: 0.4607 | Accuracy: 0.5239\n",
            "Epoch 1/16 | Batch 500/1009 | Loss: 0.4954 | Accuracy: 0.5204\n",
            "Epoch 1/16 | Batch 600/1009 | Loss: 0.4488 | Accuracy: 0.5741\n",
            "Epoch 1/16 | Batch 700/1009 | Loss: 0.4863 | Accuracy: 0.5829\n",
            "Epoch 1/16 | Batch 800/1009 | Loss: 0.4402 | Accuracy: 0.6099\n",
            "Epoch 1/16 | Batch 900/1009 | Loss: 0.4382 | Accuracy: 0.6265\n",
            "Epoch 1/16 | Batch 1000/1009 | Loss: 0.4375 | Accuracy: 0.6396\n",
            "Epoch 2/16 | Batch 0/1009 | Loss: 0.4551 | Accuracy: 0.6464\n",
            "Epoch 2/16 | Batch 100/1009 | Loss: 0.4466 | Accuracy: 0.6543\n",
            "Epoch 2/16 | Batch 200/1009 | Loss: 0.4540 | Accuracy: 0.6633\n",
            "Epoch 2/16 | Batch 300/1009 | Loss: 0.4182 | Accuracy: 0.6517\n",
            "Epoch 2/16 | Batch 400/1009 | Loss: 0.4300 | Accuracy: 0.6526\n",
            "Epoch 16/16 | Batch 0/1009 | Loss: 0.4300 | Accuracy: 0.6856\n",
            "Epoch 16/16 | Batch 100/1009 | Loss: 0.4300 | Accuracy: 0.6626\n",
            "Epoch 16/16 | Batch 200/1009 | Loss: 0.4300 | Accuracy: 0.6626\n",
            "Epoch 16/16 | Batch 300/1009 | Loss: 0.4300 | Accuracy: 0.6726\n",
            "Epoch 16/16 | Batch 400/1009 | Loss: 0.4300 | Accuracy: 0.6826\n",
            "Epoch 16/16 | Batch 500/1009 | Loss: 0.4300 | Accuracy: 0.6826\n",
            "Epoch 16/16 | Batch 600/1009 | Loss: 0.4300 | Accuracy: 0.9626\n",
            "Epoch 16/16 | Batch 700/1009 | Loss: 0.4300 | Accuracy: 0.706\n",
            "Epoch 16/16 | Batch 800/1009 | Loss: 0.4300 | Accuracy: 0.7026\n",
            "Epoch 16/16 | Batch 900/1009 | Loss: 0.4300 | Accuracy: 0.7126\n",
            "Epoch 16/16 | Batch 1000/1009 | Loss: 0.4300 | Accuracy: 0.7226\n",
            "Training Loss: 0.4300 | Training Accuracy: 0.7226\n",
            "Validation Loss: 0.4305 | Validation Accuracy: 0.7319\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate classification metrics\n",
        "classification_metrics = classification_report(true_labels, predicted_labels)\n",
        "confusion_mtx = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDt-HArQmMYL",
        "outputId": "544a2e1d-a420-4bc2-98db-973cc4682734"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.72      0.70      0.66       265\n",
            "        fear       0.71      0.71      0.67       245\n",
            "         joy       0.68      0.75      0.71       694\n",
            "        love       0.66      0.77      0.69       169\n",
            "     sadness       0.65      0.71      0.68       563\n",
            "    surprise       0.66      0.72      0.67        64\n",
            "\n",
            "    accuracy                          0.72      2000\n",
            "   macro avg       0.58      0.72      0.42      2000\n",
            "weighted avg       0.60      0.45      0.51      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[247   8   0   1   9   0]\n",
            " [  5 222   1   0   4  13]\n",
            " [  3   3 624  56   6   2]\n",
            " [  3   0   4 162   0   0]\n",
            " [  7  12   2   0 541   1]\n",
            " [  0   6   0   0   0  58]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4TMzjNEmqIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQuWw1wamqGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfCvFWDnmqD1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}